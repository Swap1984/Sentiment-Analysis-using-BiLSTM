**Sentiment Analysis using BiLSTM (End-to-End NLP Pipeline)**

This project implements a production-grade sentiment analysis system using Bidirectional LSTM (BiLSTM).

It follows a clean ML pipeline architecture with preprocessing, tokenization, training, evaluation, and artifact management.

The goal is to demonstrate industry-standard NLP workflow using Python, TensorFlow/Keras, and Pandas.

## ğŸ““ Jupyter Notebook (Rendered)

If GitHub does not render the notebook properly, you can view it here:

ğŸ”— **Notebook Viewer (nbviewer)**  
https://nbviewer.org/github/Swap1984/Sentiment-Analysis-using-BiLSTM/blob/main/Notebooks/data_loading_processing.ipynb
)

## ğŸ““ Jupyter Notebook (Rendered)

If GitHub does not render the notebook properly, you can view it here:

ğŸ”— **Notebook Viewer (nbviewer)**  
https://nbviewer.org/github/Swap1984/Sentiment-Analysis-using-BiLSTM/blob/main/Notebooks/data_loading_processing.ipynb


**ğŸ“Œ Project Highlights**

End-to-end NLP pipeline (raw data â†’ trained model)

Robust text preprocessing using regex

Tokenization & padding using Keras

Label encoding for sentiment classes

BiLSTM deep learning model

Modular production-style code (src/)

Reproducible training & evaluation

Validation accuracy ~84% and external validation ~93%

RNN_new/

â”‚

â”œâ”€â”€ data/

â”‚   â”œâ”€â”€ raw/

â”‚   â”‚   â”œâ”€â”€ twitter_training.csv

â”‚   â”‚   â””â”€â”€ twitter_validation.csv

â”‚   â”‚
â”‚   â”œâ”€â”€ processed/

â”‚   â”‚   â”œâ”€â”€ train_processed.csv

â”‚   â”‚   â””â”€â”€ val_processed.csv

â”‚

â”œâ”€â”€ artifacts/

â”‚   â”œâ”€â”€ sentiment_model.h5

â”‚   â”œâ”€â”€ tokenizer.pkl

â”‚   â””â”€â”€ label_encoder.pkl

â”‚

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ __init__.py

â”‚   â”œâ”€â”€ config.py

â”‚   â”œâ”€â”€ preprocess.py

â”‚   â”œâ”€â”€ text_tokenizer.py

â”‚   â”œâ”€â”€ model.py

â”‚   â”œâ”€â”€ train.py

â”‚   â”œâ”€â”€ evaluate.py

â”‚   â””â”€â”€ utils.py

â”‚

â”œâ”€â”€ notebooks/

â”‚   â””â”€â”€ exploration.ipynb

â”‚    â””â”€â”€ results/

    â”œâ”€â”€ metrics.txt
    
    â”œâ”€â”€ classification_report.txt
    
    â”œâ”€â”€ confusion_matrix.png
    
    â”œâ”€â”€ training_history.csv
    
    â””â”€â”€ final_inference.md


â”œâ”€â”€ README.md

â”œâ”€â”€ requirements.txt

â””â”€â”€ .gitignore

**ğŸ“Š Dataset**

Source: Twitter Sentiment Dataset

Columns used:

text â†’ Input feature (X)

sentiment â†’ Target label (y)

Only the text column is tokenized and fed to the model.

The sentiment column is label-encoded and used as the prediction target


**âš™ï¸ Text Preprocessing**

Steps applied:

Remove URLs

Normalize repeated characters

Clean excessive punctuation

Normalize whitespace

Drop null values

Implemented in::src/preprocess.py


**ğŸ”  Tokenization & Encoding**

Tokenizer: Keras Tokenizer

Vocabulary size: 10,000

Sequence length: 100

Out-of-vocabulary token supported

Label encoding using LabelEncoder

Implemented in::src/text_tokenizer.py

**Artifacts saved:**

tokenizer.pkl

label_encoder.pkl


**ğŸ§  Model Architecture**

Embedding Layer

Bidirectional LSTM

Dropout Regularization

Dense Softmax Output Layer

Implemented in:src/model.py


**ğŸš€ Training Pipeline**

The training script:

Loads processed CSVs

Tokenizes text

Encodes labels

Trains BiLSTM

Saves trained model & artifacts

Run:python src/train.py


**ğŸ“ˆ Model Evaluation**

Evaluation includes:

Validation accuracy & loss

Predictions on validation dataset

Run:python src/evaluate.py


**ğŸ“¦ Artifacts Generated**

Stored in artifacts/:

Trained model (.h5)

Tokenizer

Label encoder

These are reused for inference without retraining.

ğŸ§ª Environment Setup

Create virtual environment:
python -m venv .venv


Activate:
# Windows
.venv\Scripts\activate


Install dependencies:

pip install -r requirements.txt

ğŸ How to Run the Project (Order Matters)

python src/preprocess.py

python src/train.py

python src/evaluate.py

**Evaluation matrix**

Classification Report:

              precision    recall  f1-score   support

  Irrelevant       0.92      0.93      0.93       172
    Negative       0.94      0.95      0.94       266
     Neutral       0.95      0.93      0.94       285
    Positive       0.93      0.94      0.94       277

    accuracy                           0.94      1000
   macro avg       0.94      0.94      0.94      1000
weighted avg       0.94      0.94      0.94      1000



ğŸ§‘â€ğŸ’» Author

Swapnil Sudhakar Patil

Electronics Engineer â†’ Data Scientist / GenAI Engineer

Specialized in NLP, Deep Learning, and Production ML Pipelines

ğŸ“Œ Future Improvements

FastAPI inference service

Dockerization

MLflow experiment tracking

Transformer-based models (BERT)

CI/CD pipeline


